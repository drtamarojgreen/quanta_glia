# Research Integration & Evaluation Framework

## 1. Purpose

This framework provides a set of tools to programmatically evaluate and compare text-based answers against a list of configurable, objective evaluation points. It is designed to automate the quality assessment of content generated by Large Language Models (LLMs) or other systems, enabling rapid R&D cycles and objective model-to-model comparisons.

## 2. Architecture

The framework is split into several focused modules:

-   `research_utils.py`: Handles shared utilities, primarily the loading of optional, heavy dependencies like `sentence-transformers` and `jsonschema`. It ensures the framework can run in a minimal mode if these are not installed.
-   `evaluators.py`: Contains the core logic for running a single, atomic check. The `run_evaluation_point` function acts as a dispatcher that executes the correct validation logic based on the check's `type`.
-   `scoring.py`: Provides higher-level functions to aggregate the results from individual checks. `evaluate_answer` calculates a 0-1 score for a single answer, and `compare_answers` identifies the best-performing answer from a list.
-   `research_integration.py`: Serves as a high-level entry point and provides a runnable example demonstrating how to use the framework.

## 3. Core Concepts

-   **Research Topic**: A high-level question or task that you want an answer for (e.g., "Explain the concept of TissLang").
-   **Evaluation Point**: An atomic, verifiable check used to assess the quality of an answer. Each point is a dictionary with three key fields:
    -   `text`: A human-readable description of the check (e.g., "The answer provides at least two code examples.").
    -   `type`: The kind of validator to use (e.g., `keyword`, `regex`).
    -   `params`: A dictionary of parameters required by the validator (e.g., `{"min_count": 2}`).

## 4. Available Validators

The following validator `type`s are currently implemented:

| Type          | Description                                                                    | Required `params`                                                                                             |
|---------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| `keyword`     | Checks for the presence of specific keywords (case-insensitive).               | `keywords`: A list of strings to find.<br>`min_count`: The minimum number of keywords that must be present.     |
| `regex`       | Validates the answer against a regular expression.                             | `pattern`: The regex pattern string.                                                                          |
| `length`      | Verifies the word count of the answer is within a specified range.             | `min` (optional): Minimum word count.<br>`max` (optional): Maximum word count.                                |
| `embedding`   | Checks semantic similarity against a target text using sentence-transformers.  | `target`: The string to compare against.<br>`threshold`: A float (0-1) for the minimum required similarity. |
| `json_schema` | Validates that the answer is a valid JSON string conforming to a given schema. | `schema`: A dictionary representing the JSON schema.                                                          |
| `citation`    | Checks for the presence of citation markers (e.g., `[1]`).                     | `min_count` (optional): Minimum number of citations.<br>`pattern` (optional): A regex for the citation format. |
| `code_test`   | (Not Implemented) Safely executes code and checks the output.                  | -                                                                                                             |

## 5. Usage Example

```python
from scoring import evaluate_answer, compare_answers

# 1. Define the evaluation points for your research topic
evaluation_points = [
    {"text": "Mentions the Transformer model.",
     "type": "keyword",
     "params": {"keywords": ["Transformer"], "min_count": 1}},
    {"text": "Contains at least 2 citations.",
     "type": "citation",
     "params": {"min_count": 2}}
]

# 2. Get answers from different models
answers = [
    "The paper introduced the Transformer model [1].", # Model A
    "The Transformer model is based on attention [1] and was a breakthrough [2]." # Model B
]

# 3. Compare the answers to find the best one
winner_index, score_details = compare_answers(answers, [evaluation_points] * len(answers))

print(f"The best answer is from Model {chr(ord('A') + winner_index)}")
# Output: The best answer is from Model B

# You can also inspect the detailed results
for i, (score, details) in enumerate(score_details):
    print(f"\nModel {chr(ord('A') + i)} Score: {score:.2f}")
    for result in details:
        status = 'PASS' if result['ok'] else 'FAIL'
        print(f"  - {status}: {result['point']} ({result['note']})")
```

## 6. Extending the Framework

To add a new validator:
1.  Add a new `if t == 'my_new_validator':` block to `run_evaluation_point` in `evaluators.py`.
2.  Implement your logic within this block, reading any necessary values from `p` (the `params` dictionary).
3.  Return a `(bool, str)` tuple indicating pass/fail and a diagnostic note.
4.  Add corresponding unit tests to `tests/unit/test_research_integration.py`.

## 7. Possible Enhancements

-   **Implement `code_test` validator**: The framework includes examples for a `code_test` validator, but the logic is not yet implemented in `evaluators.py`. This would involve safely executing a code snippet (e.g., in a container or a restricted environment) and checking its `stdout`, `stderr`, and `exit_code`.
-   **Add `yaml_schema` validator**: Similar to the existing `json_schema`, this would validate YAML output, which is common in configuration and data serialization.
-   **External Configuration**: Instead of defining `evaluation_points` in Python code, they could be loaded from a structured file like `YAML` or `JSON`. This would allow non-developers to define and manage evaluation criteria.
-   **Weighted Scoring**: The current scoring system treats all evaluation points equally. An enhancement would be to add an optional `weight` parameter to each point, allowing for more nuanced and important criteria to have a greater impact on the final score.
-   **Asynchronous Evaluation**: For validators that are I/O-bound or slow (e.g., `embedding`, or a hypothetical validator that calls an external API), using `asyncio` could significantly speed up the evaluation of a single answer by running checks concurrently.
-   **Extensible Validator Registry**: The current `if/elif` chain in `evaluators.py` could be replaced with a more extensible design pattern, such as a decorator-based registry. This would make it cleaner and safer to add new validators without modifying the core dispatcher logic.
-   **Batch Processing & Reporting**: A high-level utility could be added to run evaluations over an entire dataset (e.g., a CSV file mapping research topics to multiple answers) and generate a summary report in a structured format like CSV or JSON, facilitating large-scale model comparisons.