# Research Integration & Evaluation Framework

## 1. Purpose

This framework provides a set of tools to programmatically evaluate and compare text-based answers against a list of configurable, objective evaluation points. It is designed to automate the quality assessment of content generated by Large Language Models (LLMs) or other systems, enabling rapid R&D cycles and objective model-to-model comparisons.

## 2. Architecture

The framework is split into several focused modules:

-   `research_utils.py`: Handles shared utilities, primarily the loading of optional, heavy dependencies like `sentence-transformers` and `jsonschema`. It ensures the framework can run in a minimal mode if these are not installed.
-   `evaluators.py`: Contains the core logic for running a single, atomic check. The `run_evaluation_point` function acts as a dispatcher that executes the correct validation logic based on the check's `type`.
-   `scoring.py`: Provides higher-level functions to aggregate the results from individual checks. `evaluate_answer` calculates a 0-1 score for a single answer, and `compare_answers` identifies the best-performing answer from a list.
-   `research_integration.py`: Serves as a high-level entry point and provides a runnable example demonstrating how to use the framework.

## 3. Core Concepts

-   **Research Topic**: A high-level question or task that you want an answer for (e.g., "Explain the concept of TissLang").
-   **Evaluation Point**: An atomic, verifiable check used to assess the quality of an answer. Each point is a dictionary with three key fields:
    -   `text`: A human-readable description of the check (e.g., "The answer provides at least two code examples.").
    -   `type`: The kind of validator to use (e.g., `keyword`, `regex`).
    -   `params`: A dictionary of parameters required by the validator (e.g., `{"min_count": 2}`).

## 4. Available Validators

The following validator `type`s are currently implemented:

| Type          | Description                                                                    | Required `params`                                                                                             |
|---------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| `keyword`     | Checks for the presence of specific keywords (case-insensitive).               | `keywords`: A list of strings to find.<br>`min_count`: The minimum number of keywords that must be present.     |
| `regex`       | Validates the answer against a regular expression.                             | `pattern`: The regex pattern string.                                                                          |
| `length`      | Verifies the word count of the answer is within a specified range.             | `min` (optional): Minimum word count.<br>`max` (optional): Maximum word count.                                |
| `embedding`   | Checks semantic similarity against a target text using sentence-transformers.  | `target`: The string to compare against.<br>`threshold`: A float (0-1) for the minimum required similarity. |
| `json_schema` | Validates that the answer is a valid JSON string conforming to a given schema. | `schema`: A dictionary representing the JSON schema.                                                          |
| `citation`    | Checks for the presence of citation markers (e.g., `[1]`).                     | `min_count` (optional): Minimum number of citations.<br>`pattern` (optional): A regex for the citation format. |
| `code_test`   | Safely executes a Python code snippet in a Docker container and checks its output. | `stdout` (optional): The expected standard output.<br>`exit_code` (optional): The expected exit code (default 0).<br>`image` (optional): The Docker image to use (default `python:3.9-slim`). |

## 5. Usage Example

```python
from scoring import evaluate_answer, compare_answers

# 1. Define the evaluation points for your research topic
evaluation_points = [
    {"text": "Mentions the Transformer model.",
     "type": "keyword",
     "params": {"keywords": ["Transformer"], "min_count": 1}},
    {"text": "Contains at least 2 citations.",
     "type": "citation",
     "params": {"min_count": 2}}
]

# 2. Get answers from different models
answers = [
    "The paper introduced the Transformer model [1].", # Model A
    "The Transformer model is based on attention [1] and was a breakthrough [2]." # Model B
]

# 3. Compare the answers to find the best one
winner_index, score_details = compare_answers(answers, [evaluation_points] * len(answers))

print(f"The best answer is from Model {chr(ord('A') + winner_index)}")
# Output: The best answer is from Model B

# You can also inspect the detailed results
for i, (score, details) in enumerate(score_details):
    print(f"\nModel {chr(ord('A') + i)} Score: {score:.2f}")
    for result in details:
        status = 'PASS' if result['ok'] else 'FAIL'
        print(f"  - {status}: {result['point']} ({result['note']})")
```

## 6. Extending the Framework

To add a new validator:
1.  Add a new `if t == 'my_new_validator':` block to `run_evaluation_point` in `evaluators.py`.
2.  Implement your logic within this block, reading any necessary values from `p` (the `params` dictionary).
3.  Return a `(bool, str)` tuple indicating pass/fail and a diagnostic note.
4.  Add corresponding unit tests to `tests/unit/test_research_integration.py`.

## 7. Future Enhancements

The framework is designed for extensibility. Here are some potential future enhancements that build on the existing foundation:

-   **Performance Logging**: Augment the evaluation results to include the execution time for each individual check. This data would help identify performance bottlenecks in the evaluation suite, especially for resource-intensive validators like `embedding` or `code_test`.
-   **Report Generation**: The ability to export evaluation results into simple, portable formats like CSV or Markdown. This would make it easy to analyze results in other tools or share them in reports without requiring complex library dependencies.
-   **Weighted Scoring**: Enhance the `scoring.py` module to allow an optional `weight` parameter for each evaluation point. This would allow critical checks to have a greater impact on the final score than minor ones, providing more nuanced quality assessment.
-   **Conditional Evaluation**: Introduce an optional `id` and `depends_on` field to evaluation points. A check would only run if the evaluation point `id` referenced in `depends_on` has passed. This enables dependent, tree-like evaluation logic (e.g., "First, check if a list is present. If so, then check if the list has at least 3 items.").
-   **Negation Validator**: A new validator type (`negation`) that passes only if a specified list of keywords is *not* found in the answer. This is useful for ensuring answers avoid certain terminology or concepts.
-   **Enhanced CLI**: Develop a more robust command-line interface using Python's built-in `argparse` module. This would allow users to specify input files for answers, evaluation configurations, and report outputs directly from the terminal, making the tool more flexible for automated workflows.
-   **Directory-based Batch Processing**: Enhance the CLI to accept a directory path. The script could automatically discover and group answer files based on a naming convention (e.g., `topic1_modelA.txt`, `topic1_modelB.txt`), allowing for large-scale batch evaluations without manual script editing.
-   **Interactive Debugging Mode**: Add a command-line flag (e.g., `--debug-on-fail`) that, upon a validation failure, drops the user into an interactive Python debugger (`pdb`) session. This would allow for immediate inspection of the answer text and validator parameters to understand the cause of the failure.
-   **Golden Answer Diffing**: A new validator (`diff`) that compares the generated answer against a canonical "golden answer" using Python's built-in `difflib`. This would report on insertions, deletions, and changes, providing a powerful way to detect regressions or deviations from a known-good response.
-   **Pre-evaluation Normalizers**: Introduce an optional `normalizers` list to the evaluation configuration. These would be simple functions (e.g., `to_lowercase`, `strip_markdown`) applied to the answer text *before* any validators are run. This ensures that checks are performed on a consistent, clean version of the content.
-   **Informational-Only Checks**: Add a boolean `informational` flag to evaluation points. When set to `true`, the check will run and its result will be included in the detailed report, but it will not affect the final numeric score. This is useful for gathering metadata or running non-critical diagnostics.
-   **Meta-Evaluation Validator**: A new validator (`meta_eval`) that dynamically generates and injects new evaluation points into the current run based on the answer's content. For example, if an answer mentions a specific technical term, this validator could add a new, more granular check like, "The definition of the discovered term must be accurate." This allows the evaluation to adapt and deepen its inquiry in response to the provided text.
-   **Directive Feedback Generation**: Add an optional `suggestion` field to evaluation points. On failure, the framework would format this string (e.g., "The answer failed the length check with {actual} words. Consider elaborating on the core concepts.") and include it in the report. This shifts the output from a simple grade to actionable, directive feedback for improvement.
-   **Consensus-based Benchmark Synthesis**: A post-processing step that, after evaluating a batch of answers, attempts to construct a new "golden answer" by combining the specific sentences or paragraphs from different models that passed the most critical checks. This creates an "evolving benchmark" that represents the best-of-breed synthesis from the evaluated content, reflecting a more holistic understanding of a high-quality response.
-   **Evidence Snippets in Reports**: Enhance validators to return the specific text snippet(s) that caused a check to pass or fail. For example, a `keyword` check would list the found words, and a `regex` check would show the matched string. This provides direct, traceable evidence for each evaluation result, moving beyond a simple PASS/FAIL status.
-   **Categorical Score Aggregation**: Introduce an optional `category` tag (e.g., 'Clarity', 'Completeness', 'Safety') to evaluation points. The final report would then include sub-scores for each category, offering a multi-dimensional view of an answer's strengths and weaknesses instead of a single, monolithic score.
-   **Side-by-Side Comparative Reporting**: Generate a detailed HTML or Markdown report that presents a side-by-side comparison of multiple answers. Each row in the report would correspond to an evaluation point, making it easy to visually pinpoint specific criteria where one model outperforms another.
-   **CI/CD Integration**: A dedicated script or action to integrate the evaluation framework into a CI/CD pipeline, similar to the `test_workspace.sh` script for repository-level testing. This would allow for automatic regression testing of LLM prompts and models on every code change.
-   **Configuration from JSON**: Allow `evaluation_points` to be defined in and loaded from an external `.json` file. This decouples the test criteria from the execution script, making it easier to manage, version, and share large sets of evaluation rules.
-   **Validator Chaining**: Introduce the ability to chain validators. For example, one could first use a `regex` validator to extract a JSON block from the text, and then pass that extracted block to the `json_schema` validator. This would enable more complex, multi-step validation scenarios.
